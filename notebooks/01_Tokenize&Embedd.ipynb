{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNO5vTQBpM0TRx8mNYCgGn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken -q"
      ],
      "metadata": {
        "id": "cqsX2IHjsyUo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "_4ulR6gKnyhx"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from typing import List, Dict, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./the-verdict.txt\", 'r', encoding='utf-8') as f:\n",
        "  raw_text = f.read()"
      ],
      "metadata": {
        "id": "0iLO7TTrcNmt"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item for item in preprocessed if item.strip()]\n",
        "len(preprocessed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfWrrOigtgqR",
        "outputId": "c3ab1b04-13d7-48d3-e1f2-2dd971490ee4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4649"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(list(set([x.lower() for x in preprocessed])))\n",
        "temp = enumerate(vocab)\n",
        "encoder = {k:v for v,k in temp}\n",
        "decoder = {v:k for k,v in encoder.items()}"
      ],
      "metadata": {
        "id": "qWHst3d70paI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = [encoder[x.lower()] for x in preprocessed]\n",
        "decoded = [decoder[x] for x in encoded]\n",
        "print(encoded[:20])\n",
        "print(decoded[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5jjCh1E06CG",
        "outputId": "b3db7c4a-74f4-4bfa-c0cf-000f3d8f52a2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[490, 436, 47, 966, 517, 405, 767, 12, 161, 400, 6, 965, 12, 417, 347, 303, 6, 868, 513, 1047]\n",
            "['i', 'had', 'always', 'thought', 'jack', 'gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "  def __init__(self, raw_text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
        "    preprocessed = [item for item in preprocessed if item.strip()]\n",
        "    vocab = sorted(list(set([x.lower() for x in preprocessed])))\n",
        "    self.encoder = {k:v for v,k in enumerate(vocab)}\n",
        "    self.decoder = {v:k for k,v in self.encoder.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item for item in preprocessed if item.strip()]\n",
        "    return [self.encoder[word.lower()] for word in preprocessed]\n",
        "\n",
        "  def decode(self, ids):\n",
        "    return \" \".join([self.decoder[id] for id in ids])"
      ],
      "metadata": {
        "id": "cjLO3HSZ06Xe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, raw_text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
        "    preprocessed = [item for item in preprocessed if item.strip()]\n",
        "    vocab = sorted(list(set([x.lower() for x in preprocessed])))\n",
        "\n",
        "    self.encoder = {key: value for value, key in enumerate(vocab)}\n",
        "    self.encoder[\"<unk>\"], self.encoder[\"<EOF>\"] = len(vocab), len(vocab) + 1\n",
        "    self.decoder = {value: key for key, value in self.encoder.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.lower() for item in preprocessed if item.strip()]\n",
        "    return [self.encoder[word] if word in self.encoder else self.encoder[\"<unk>\"] for word in preprocessed] + [self.encoder[\"<EOF>\"]]\n",
        "\n",
        "  def decode(self, ids):\n",
        "    return \" \".join([self.decoder[id] for id in ids])"
      ],
      "metadata": {
        "id": "TbY8QHwcoBwp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok = SimpleTokenizerV2(raw_text)\n",
        "ids = tok.encode(\"I am a loser\")\n",
        "words = tok.decode(ids)\n",
        "print(ids)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlgmNmur06kg",
        "outputId": "5872bdde-e50f-4b5b-ba85-c29b99bb85bf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[490, 48, 12, 1105, 1106]\n",
            "i am a <unk> <EOF>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "bXa_45NH06q1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Ich kann es nicht tun\"\n",
        "ids = tokenizer.encode(text)\n",
        "words = tokenizer.decode(ids)\n",
        "print(text)\n",
        "print(ids)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUjLHCwz06wi",
        "outputId": "58c228c9-d272-4ef3-feac-4d86dbc0bad3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ich kann es nicht tun\n",
            "[40, 354, 479, 1236, 1658, 299, 30830, 6278]\n",
            "Ich kann es nicht tun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_text = tokenizer.encode(raw_text)\n",
        "enc_sample = enc_text[:50]\n",
        "temp = torch.tensor(enc_sample)\n",
        "temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsYkcIFY063L",
        "outputId": "bd3a87a1-d53e-46f2-c830-f3907c0d6d94"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   40,   367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,\n",
              "          257,  7026, 15632,   438,  2016,   257,   922,  5891,  1576,   438,\n",
              "          568,   340,   373,   645,  1049,  5975,   284,   502,   284,  3285,\n",
              "          326,    11,   287,   262,  6001,   286,   465, 13476,    11,   339,\n",
              "          550,  5710,   465, 12036,    11,  6405,   257,  5527, 27075,    11])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTextDataset(Dataset):\n",
        "  def __init__(self, text, tokenizer, context_length):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.ids = torch.tensor(tokenizer.encode(text))\n",
        "    self.context_length = context_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ids)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    assert index + self.context_length < len(self)\n",
        "    x_sample = self.ids[index: index + self.context_length]\n",
        "    y_sample = self.ids[self.context_length + index]\n",
        "    return x_sample, y_sample"
      ],
      "metadata": {
        "id": "uGwfHmA_dOlV"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self, text, tokenizer, max_length, stride):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "    ids = self.tokenizer.encode(text)\n",
        "\n",
        "    for i in range(0, len(ids) - max_length, stride):\n",
        "      input_chunk = ids[i: i + max_length]\n",
        "      target_chunk = ids[i+1 : i + max_length + 1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.input_ids[index], self.target_ids[index]"
      ],
      "metadata": {
        "id": "D-e-GQOdhGB3"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = CustomTextDataset(raw_text, tokenizer, 5)\n",
        "train_dl = DataLoader(train_ds, batch_size=4)\n",
        "batch = next(iter(train_dl))"
      ],
      "metadata": {
        "id": "4j4rT5jJdOsF"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = GPTDatasetV1(raw_text, tokenizer, 5, 1)\n",
        "train_dl = DataLoader(train_ds, batch_size=4)\n",
        "xb, yb = next(iter(train_dl))"
      ],
      "metadata": {
        "id": "V42D0sUVdOzb"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_embedding = 50257\n",
        "embedding_dim = 256\n",
        "embedding_layer = torch.nn.Embedding(num_embedding, embedding_dim)"
      ],
      "metadata": {
        "id": "4mOPrSdVIZ1c"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fYYBLZ8TIZ-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5TvcuPnAIaIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f-yAg2KHIaTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "str_to_int = {s:i for i,s in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "4_bANGuQq_lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Basic_Tokenizer:\n",
        "  def __init__(self, vocab: List):\n",
        "    self.vocab = sorted(list(set(vocab)))\n",
        "    self.str_to_int = {s:i for i,s in enumerate(self.vocab)}\n",
        "    self.str_to_int[\"<|unk|>\"] = len(self.vocab)\n",
        "    self.str_to_int[\"<|EOT|>\"] = len(self.vocab) + 1\n",
        "    self.int_to_str = {i:s for s,i in self.str_to_int.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item for item in preprocessed if item.strip()]\n",
        "    token_IDs = [self.str_to_int[word] if word in self.vocab else self.str_to_int[\"<|unk|>\"] for word in preprocessed] + [self.str_to_int[\"<|EOT|>\"]]\n",
        "    return token_IDs\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text =  \" \".join([self.int_to_str[token] for token in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "QYA0hNsKrqyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok = Basic_Tokenizer(preprocessed)\n",
        "tokens = tok.encode(\"I am a loser. I am that, mon frere.\")\n",
        "tok.decode(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "K6f-b5v9vSKZ",
        "outputId": "49fa46a9-da0f-4c06-cba6-fb98a4a45cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am a <|unk|>. I am that, <|unk|> <|unk|>. <|EOT|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# alltokens = sorted(list(set(preprocessed)))\n",
        "# str_to_int = {s:i for i,s in enumerate(alltokens)}"
      ],
      "metadata": {
        "id": "ee4XigqZ17x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens, preprocessed_ids = np.unique(preprocessed, return_inverse=True)\n",
        "len(all_tokens), len(preprocessed_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mT-_CiTtiJ5",
        "outputId": "a53e71bb-4ed5-4500-85ed-a1d1b16eb824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1159, 4649)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens[preprocessed_ids[:10]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvLdnq2Fu-QY",
        "outputId": "58185c83-1bfa-4c09-a5c6-623622f5441c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a',\n",
              "       'cheap', 'genius'], dtype='<U18')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicTokenizerV1:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {value: key for key, value in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item for item in preprocessed if item.strip()]\n",
        "    # alltokens = sorted(list(set(preprocessed)))\n",
        "    # str_to_int = {s:i for i,s in enumerate(alltokens)}\n",
        "    token_ids = [self.str_to_int[token] for token in preprocessed]\n",
        "    return token_ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[id] for id in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "yDt3s-3RyenA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alltokens = sorted(list(set(preprocessed)))\n",
        "str_to_int = {s:i for i,s in enumerate(alltokens)}\n",
        "BT1 = BasicTokenizerV1(str_to_int)"
      ],
      "metadata": {
        "id": "dTYNk8uI3Xa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = BT1.encode(raw_text)\n",
        "text = BT1.decode(ids)"
      ],
      "metadata": {
        "id": "-0SDmgoG3roR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {value: key for key, value in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item for item in preprocessed if item.strip()]\n",
        "    token_ids = [self.str_to_int[token] if token in self.str_to_int else self.str_to_int[\"<|unk|>\"] for token in preprocessed]\n",
        "    # token_ids.append(self.str_to_int[\"<|endoftext|>\"])\n",
        "    return token_ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[id] for id in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "6v9Rvfj730ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alltokens = sorted(list(set(preprocessed)))\n",
        "alltokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "str_to_int = {s:i for i,s in enumerate(alltokens)}\n",
        "BT2 = BasicTokenizerV2(str_to_int)"
      ],
      "metadata": {
        "id": "zwI3FcrFNvk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello World, is this ann?\"\n",
        "BT2.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Sm94K86N9a5",
        "outputId": "4f1fc838-6f14-4fb2-d7e9-493a1f023a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1160, 1160, 5, 595, 1024, 1160, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pem-yDWyP5p4",
        "outputId": "8216011b-c2d4-422c-87c3-de04f700f15c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(BT2.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA8GVpzoTF8L",
        "outputId": "db2b8905-2824-472a-cca5-93ba3d254213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "wuTnV9j4TOn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ints = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(ints)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMhfinKlUR8F",
        "outputId": "799ecabc-fae1-4707-f006-176cf770fd78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 262, 20562, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ints)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NmPayP-NXK2v",
        "outputId": "73cd1d1e-af15-4ddc-e5e1-a823976e971f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gu4gJ6OgpkWl",
        "outputId": "b5911240-1358-4617-c7ae-b03841870666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_text = tokenizer.encode(raw_text)"
      ],
      "metadata": {
        "id": "FEXWZa2OXcBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(enc_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2D_1R9asnGb",
        "outputId": "bf60fdd0-ad56-4a7e-db3f-a1c928d85f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5145"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[50:]\n",
        "dec_sample = tokenizer.decode(enc_sample)\n",
        "dec_sample[:20]"
      ],
      "metadata": {
        "id": "JFONX8b_sopG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2747f91c-b977-4a5a-a546-be71f26e1c42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' and established him'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 5\n",
        "length = 25\n",
        "x_samples, y_samples = [], []\n",
        "\n",
        "for i in range(length):\n",
        "  x_sample, y_sample = enc_sample[i: i+context_size], enc_sample[i+context_size:i+context_size+1]\n",
        "  x_samples.append(x_sample)\n",
        "  y_samples.append(y_sample)\n",
        "\n",
        "print(x_samples)\n",
        "print(y_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM6HkJzoqSs1",
        "outputId": "17324626-1188-4ee6-a20d-fc25900149ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[290, 4920, 2241, 287, 257], [4920, 2241, 287, 257, 4489], [2241, 287, 257, 4489, 64], [287, 257, 4489, 64, 319], [257, 4489, 64, 319, 262], [4489, 64, 319, 262, 34686], [64, 319, 262, 34686, 41976], [319, 262, 34686, 41976, 13], [262, 34686, 41976, 13, 357], [34686, 41976, 13, 357, 10915], [41976, 13, 357, 10915, 314], [13, 357, 10915, 314, 2138], [357, 10915, 314, 2138, 1807], [10915, 314, 2138, 1807, 340], [314, 2138, 1807, 340, 561], [2138, 1807, 340, 561, 423], [1807, 340, 561, 423, 587], [340, 561, 423, 587, 10598], [561, 423, 587, 10598, 393], [423, 587, 10598, 393, 28537], [587, 10598, 393, 28537, 2014], [10598, 393, 28537, 2014, 198], [393, 28537, 2014, 198, 198], [28537, 2014, 198, 198, 1], [2014, 198, 198, 1, 464]]\n",
            "[[4489], [64], [319], [262], [34686], [41976], [13], [357], [10915], [314], [2138], [1807], [340], [561], [423], [587], [10598], [393], [28537], [2014], [198], [198], [1], [464], [6001]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KLv658DeCP3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoqCjTi2rHJn",
        "outputId": "14603780-5957-488e-9224-887da8b15411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt)\n",
        "\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "      input_chunk = token_ids[i: i+max_length]\n",
        "      output_chunk = token_ids[i+1: i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(output_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "u95u-TEorHQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(txt, batch_size: int=4, max_length: int=256, stride: int=128, shuffle: bool=True, drop_last: bool=True):\n",
        "  tokenizer = tiktoken.get_encoding('gpt2')\n",
        "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "d_ZTabp4rHW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl = create_dataloader(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)"
      ],
      "metadata": {
        "id": "vVTlkMk4rHda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for xb,yb in dl:\n",
        "  print(xb, \": \", yb)\n",
        "  count+=1\n",
        "  if count >= 3:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvsWtQoKrHke",
        "outputId": "cfa4e919-3bc7-400b-c731-abed9db6f556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  40,  367, 2885, 1464]]) :  tensor([[ 367, 2885, 1464, 1807]])\n",
            "tensor([[ 367, 2885, 1464, 1807]]) :  tensor([[2885, 1464, 1807, 3619]])\n",
            "tensor([[2885, 1464, 1807, 3619]]) :  tensor([[1464, 1807, 3619,  402]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size, output_dim = 6, 3"
      ],
      "metadata": {
        "id": "5GaL3MS0rHrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "zbti1xbVXMep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WVO874JXMkf",
        "outputId": "9517ab6a-1de1-4022-e997-dd9dd76ecdef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.3746,  1.4903,  0.5155],\n",
              "        [ 1.3757,  2.1852, -0.5437],\n",
              "        [-1.2318, -0.6525, -0.3105],\n",
              "        [-0.4463,  0.8960, -0.0458],\n",
              "        [-0.9874, -0.7083, -0.6746],\n",
              "        [-1.0927,  0.1884,  1.3215]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dim = 256\n",
        "vocab_size = 50257"
      ],
      "metadata": {
        "id": "xPKkiC5PXMp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "uo3RIcGFXMuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4"
      ],
      "metadata": {
        "id": "20FSBl8EXMy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl = create_dataloader(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)"
      ],
      "metadata": {
        "id": "wdKUb1HjXM22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, targets = next(iter(dl))"
      ],
      "metadata": {
        "id": "FCUruBbQXM6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf420ASYXM-s",
        "outputId": "c0330f86-6131-4561-d5a6-31dcc3ed2a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[   40,   367,  2885,  1464],\n",
              "         [ 1807,  3619,   402,   271],\n",
              "         [10899,  2138,   257,  7026],\n",
              "         [15632,   438,  2016,   257],\n",
              "         [  922,  5891,  1576,   438],\n",
              "         [  568,   340,   373,   645],\n",
              "         [ 1049,  5975,   284,   502],\n",
              "         [  284,  3285,   326,    11]]),\n",
              " tensor([[  367,  2885,  1464,  1807],\n",
              "         [ 3619,   402,   271, 10899],\n",
              "         [ 2138,   257,  7026, 15632],\n",
              "         [  438,  2016,   257,   922],\n",
              "         [ 5891,  1576,   438,   568],\n",
              "         [  340,   373,   645,  1049],\n",
              "         [ 5975,   284,   502,   284],\n",
              "         [ 3285,   326,    11,   287]]))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBHXLrufXNCx",
        "outputId": "696fe878-12c2-45f0-a96c-60d1ca51a5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "token_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chHrjW-CXNGz",
        "outputId": "3f7a6eea-7c57-4316-b75e-2481a79db556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 4, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ggAOfoveZ59h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "etaP36XQZ6DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DGn2m8oNZ6JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hs-Q0G0bZ6PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j1PqQVneZ6Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6rRy4bWtZ6bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VDgsbD85Z6hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lbzqj7CcZ6m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sDIyw2XaZ6uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 5\n",
        "length = 25\n",
        "x_samples = []\n",
        "y_samples = []\n",
        "\n",
        "for i in range(length):\n",
        "  x = enc_sample[i:i+context_size]\n",
        "  y = enc_sample[i+context_size: i+context_size+1]\n",
        "  x_samples.append(x)\n",
        "  y_samples.append(y)\n",
        "\n",
        "\n",
        "for i in range(length):\n",
        "  print(tokenizer.decode(x_samples[i]), \": \", tokenizer.decode(y_samples[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eERGvCJt0cO",
        "outputId": "99d05fc4-12fd-4548-d7da-4ee96cdf1859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and established himself in a :   vill\n",
            " established himself in a vill :  a\n",
            " himself in a villa :   on\n",
            " in a villa on :   the\n",
            " a villa on the :   Riv\n",
            " villa on the Riv :  iera\n",
            "a on the Riviera :  .\n",
            " on the Riviera. :   (\n",
            " the Riviera. ( :  Though\n",
            " Riviera. (Though :   I\n",
            "iera. (Though I :   rather\n",
            ". (Though I rather :   thought\n",
            " (Though I rather thought :   it\n",
            "Though I rather thought it :   would\n",
            " I rather thought it would :   have\n",
            " rather thought it would have :   been\n",
            " thought it would have been :   Rome\n",
            " it would have been Rome :   or\n",
            " would have been Rome or :   Florence\n",
            " have been Rome or Florence :  .)\n",
            " been Rome or Florence.) :  \n",
            "\n",
            " Rome or Florence.)\n",
            " :  \n",
            "\n",
            " or Florence.)\n",
            "\n",
            " :  \"\n",
            " Florence.)\n",
            "\n",
            "\" :  The\n",
            ".)\n",
            "\n",
            "\"The :   height\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GEKxiXclvKxF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}