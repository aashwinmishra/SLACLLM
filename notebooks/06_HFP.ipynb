{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5hDiXwbkoJq3cmiTZmXO9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdBhYfrRBOEZ",
        "outputId": "9dc6ae0f-bb7c-4417-f96c-fd476de56858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed (Y/n)? Y\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires pyarrow<18.0.0a0,>=14.0.0, but you have pyarrow 18.0.0 which is incompatible.\n",
            "pylibcudf-cu12 24.10.1 requires pyarrow<18.0.0a0,>=14.0.0, but you have pyarrow 18.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires pyarrow<18.0.0a0,>=14.0.0, but you have pyarrow 18.0.0 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip uninstall pyarrow -q\n",
        "!pip install --upgrade pyarrow -q\n",
        "!pip install transformers -q\n",
        "!pip install datasets -q\n",
        "!pip install evaluate -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoModelForCausalLM, AutoModelForMaskedLM\n",
        "from transformers import BertModel, BertConfig, BertTokenizer\n",
        "from transformers import AdamW\n",
        "from transformers import get_scheduler\n",
        "from transformers import DataCollatorWithPadding, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import evaluate\n",
        "\n",
        "import wandb\n",
        "wandb.init(mode='disabled')\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import html\n",
        "import requests"
      ],
      "metadata": {
        "id": "ntzv1Ie-BVks"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
        "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
        "\n",
        "raw_datasets = DatasetDict(\n",
        "    {\n",
        "        \"train\": ds_train.shuffle().select(range(50000)),\n",
        "        \"valid\": ds_valid.shuffle().select(range(500))\n",
        "    }\n",
        ")\n",
        "\n",
        "raw_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKIu4fD6BVri",
        "outputId": "babc650a-d058-4b5f-dae3-00d1ac195cb8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets[\"train\"]['content'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "zSiB25mNBVwl",
        "outputId": "595d11ff-2a1b-4fbd-f3b5-31c02262dd80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import pandas as pd\\nimport numpy as np\\nimport pyarrow as pa\\nimport pyarrow.parquet as pq\\nfrom fastparquet import ParquetFile\\nimport os\\nfrom stringdist import levenshtein_norm as lev_norm\\n\\n\\n\\nclass Preprocessor(object):\\n    def __init__(self, pq_file_dir, output, mimic_notes):\\n        self.pq_file_dir = pq_file_dir\\n        self.output = output\\n        self.mimic_notes_file = mimic_notes\\n\\n        pf = ParquetFile(self.mimic_notes_file)\\n        self.notes = pf.to_pandas()    \\n\\n        self.preds = self.get_df_from_pq(self.pq_file_dir, \\'predicates\\')\\n        self.mentions = self.get_df_from_pq(self.pq_file_dir, \\'mentions\\')\\n        self.umls = self.get_df_from_pq(self.pq_file_dir, \\'umls_concepts\\')\\n        self.sents = self.get_df_from_pq(self.pq_file_dir, \\'sentences\\')\\n        print(\"Finished loading data...\")\\n        \\n    def preprocess(self):\\n        self.sents = self.sents.rename({\\'id\\': \\'sent_id\\'}, axis=1)\\n\\n        # Add raw text from notes to sentences\\n        self.sents = self.sents.rename({\\'id\\': \\'sent_id\\'}, axis=1)\\n        self.sents = self.sents.merge(self.notes[[\\'ROW_ID\\', \\'TEXT\\']],\\n                    left_on=\\'doc_id\\', right_on=\\'ROW_ID\\').drop(\\'ROW_ID\\', axis=1)\\n\\n        self.sents = self.sents.apply(self.extract_sent, axis=1)\\n        self.sents = self.sents.rename({\\'TEXT\\': \\'text\\'}, axis=1)\\n\\n        # Add position of sentence in document to sentences df\\n        self.sents = self.set_sentence_pos(self.sents)\\n\\n        # remove sentences without entities\\n        sents_with_mentions = self.sents[\\n                    self.sents[\\'sent_id\\'].isin(\\n                        self.mentions.drop_duplicates(subset=\\'sent_id\\')[\\'sent_id\\']\\n                    )\\n                ]\\n\\n        # Remove umls concepts which don\\'t have a preferred text field\\n        self.umls = self.umls[~self.umls[\\'preferred_text\\'].isna()]\\n\\n        # Prep mentions\\n        self.mentions = self.transform_mentions(self.mentions)\\n\\n        # Add original text to mentions\\n        self.mentions[\\'text\\'] = self.mentions.apply(self.get_text_from_sentence, args=(self.notes,), axis=1)\\n\\n        # Add sentence position to mentions\\n        self.mentions = self.mentions.merge(sents_with_mentions[[\\'sent_id\\', \\'sentence_number\\']],\\n                                    on=\\'sent_id\\')\\n\\n        # Prep predicates\\n        self.preds = self.transform_preds(self.preds)\\n        # Remove predicates not in sentences with mentions\\n        self.preds = self.preds[\\n                    self.preds[\\'sent_id\\'].isin( sents_with_mentions[\\'sent_id\\'] )\\n                ]\\n        self.preds[\\'text\\'] = self.preds.apply(self.get_text_from_sentence, args=(self.notes,), axis=1)\\n\\n        # Assign cui codes to mentions (entities)\\n        self.mentions[[\\'cui\\', \\'umls_xmi_id\\', \\'preferred_text\\']] = self.mentions. \\\\\\n                                                     apply(self.get_cui, args=(self.umls,), axis=1, result_type=\\'expand\\')\\n\\n        # Set the template tokens we\\'re going to use\\n        self.mentions[\\'template_token\\'] = self.mentions[\\'mention_type\\']\\n        self.preds[\\'template_token\\'] = self.preds[\\'frameset\\']\\n\\n        preds_toks = self.preds.apply(self.get_template_tokens, axis=1)\\n        mentions_toks = self.mentions.apply(self.get_template_tokens, axis=1)   \\n\\n        # Append the two template tokens dataframes and sort \\n        template_tokens = preds_toks.append(mentions_toks)\\n        temp_tokens = template_tokens.groupby([\\'sent_id\\']).apply(lambda x: x.sort_values([\\'begin\\']))     \\n\\n        # Create semantic templates\\n        sem_templates = template_tokens.sort_values(\\'begin\\').groupby(\\'sent_id\\')[\\'token\\'].apply(\\' \\'.join)\\n\\n        sem_df = pd.DataFrame(sem_templates)\\n\\n        sem_df.reset_index(level=0, inplace=True)\\n\\n        sem_df = sem_df.rename(columns={\\'token\\': \\'sem_template\\'})\\n\\n        sem_df = sem_df.merge(self.sents[[\\'sent_id\\', \\'sentence_number\\', \\'doc_id\\', \\'begin\\', \\'end\\']],\\n                            left_on=\\'sent_id\\', right_on=\\'sent_id\\' )\\n\\n        temp_by_pos = pd.crosstab(sem_df[\\'sem_template\\'], sem_df[\\'sentence_number\\']).apply(lambda x: x / x.sum(), axis=0)\\n\\n\\n        # Store dataframes for clustering later\\n        sents_with_mentions.to_parquet(f\\'{self.output}/sentences.parquet\\')\\n        self.mentions.to_parquet(f\\'{self.output}/mentions.parquet\\')\\n        self.umls.to_parquet(f\\'{self.output}/umls.parquet\\')\\n\\n        sem_df.to_parquet(f\\'{self.output}/templates.parquet\\')\\n\\n    def get_df_from_pq(self, root, name):\\n        return pq.read_table(f\\'{root}/{name}\\').to_pandas()\\n\\n    def transform_preds(self, df):\\n        df[\\'frameset\\'] = df[\\'frameset\\'].apply(lambda x: x.split(\\'.\\')[0])\\n        return df\\n\\n    def transform_mentions(self, mentions):\\n        # Don\\'t want this to fail if these have already been removed\\n        try:\\n            mentions = mentions.drop(\\n                [\\'conditional\\', \\'history_of\\', \\'generic\\', \\'polarity\\', \\'discovery_technique\\', \\'subject\\'],\\n                axis=1)\\n        except:\\n            pass\\n        \\n        sorted_df = mentions.groupby([\\'sent_id\\', \\'begin\\']) \\\\\\n                            .apply(lambda x: x.sort_values([\\'begin\\', \\'end\\']))\\n        \\n        # Drop the mentions that are parts of a larger span.  Only keep the containing span that holds multiple\\n        # mentions\\n        deduped = sorted_df.drop_duplicates([\\'sent_id\\', \\'begin\\'], keep=\\'last\\')\\n        deduped = deduped.drop_duplicates([\\'sent_id\\', \\'end\\'], keep=\\'first\\')\\n        return deduped.reset_index(drop=True)\\n\\n    def set_template_token(self, df, column):\\n        df[\\'template_token\\'] = df[column]\\n        return df\\n\\n    def get_template_tokens(self, row):\\n        return pd.Series({\\n            \\'doc_id\\': row[\\'doc_id\\'],\\n            \\'sent_id\\': row[\\'sent_id\\'],\\n            \\'token\\': row[\\'template_token\\'],\\n            \\'begin\\': row[\\'begin\\'],\\n            \\'end\\': row[\\'end\\']\\n            })    \\n\\n    def set_sentence_pos(self, df):\\n        df = df.groupby([\"doc_id\"]).apply(lambda x: x.sort_values([\"begin\"])).reset_index(drop=True)\\n        df[\\'sentence_number\\'] = df.groupby(\"doc_id\").cumcount()\\n        return df\\n\\n    def extract_sent(self, row):\\n        begin = row[\\'begin\\']\\n        end = row[\\'end\\']\\n        row[\\'TEXT\\'] = row[\\'TEXT\\'][begin:end]\\n        return row\\n            \\n    def get_text_from_sentence(self, row, notes):\\n        doc = notes[notes[\\'ROW_ID\\'] == row[\\'doc_id\\']]\\n        b = row[\\'begin\\']\\n        e = row[\\'end\\']\\n        return doc[\\'TEXT\\'].iloc[0][b:e]        \\n\\n    def edit_dist(self, row, term2):\\n        term1 = row.loc[\\'preferred_text\\']\\n        return lev_norm(term1, term2)\\n        \\n    def get_cui(self, mention, umls_df):\\n        ont_arr = list(map(int, mention[\\'ontology_arr\\'].split())) or None\\n        ment_text = mention[\\'text\\']\\n        \\n        concepts = umls_df[umls_df[\\'xmi_id\\'].isin(ont_arr)].loc[:, [\\'cui\\', \\'preferred_text\\', \\'xmi_id\\']]\\n        concepts[\\'dist\\'] = concepts.apply(self.edit_dist, args=(ment_text,), axis=1)\\n        sorted_df = concepts.sort_values(by=\\'dist\\', ascending=True).reset_index(drop=True)\\n        cui = sorted_df[\\'cui\\'].iloc[0]\\n        xmi_id = sorted_df[\\'xmi_id\\'].iloc[0]\\n        pref_text = sorted_df[\\'preferred_text\\'].iloc[0]\\n        return cui, xmi_id, pref_text'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c6HyhjREBV1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pKS_oB8dBV5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X5GlEbuYBV9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SXElC8i0BWBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_5WcnS3RBWEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JbVhNdtwBWJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EI-kBf06BWLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A8SknydJBWOn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}