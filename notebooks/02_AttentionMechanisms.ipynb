{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqRDskyswg6PH8MBYyAJKm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8kIGf7L-Ojw",
        "outputId": "ac803ae4-fa8b-43b9-b544-ee11399c3bd9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m680.9 kB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eHMCInwcA3lC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import math\n",
        "\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "[[0.43, 0.15, 0.89], # Your (x^1)\n",
        "[0.55, 0.87, 0.66], # journey (x^2)\n",
        "[0.57, 0.85, 0.64], # starts (x^3)\n",
        "[0.22, 0.58, 0.33], # with (x^4)\n",
        "[0.77, 0.25, 0.10], # one (x^5)\n",
        "[0.05, 0.80, 0.55]] # step (x^6)\n",
        ")\n",
        "\n",
        "print(inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdlTYbMH7AMa",
        "outputId": "b08754b0-c3f6-4c20-deea-8f82fc0b6189"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimplifiedSelfAttention(nn.Module):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def forward(self, input):\n",
        "    assert len(input.shape) == 3\n",
        "    d = torch.tensor(input.shape[-1])\n",
        "    Q, K, V = input, input, input\n",
        "    attention_scores = torch.softmax(Q @ K.transpose(-1, -2)/torch.sqrt(d), dim=-1)\n",
        "    return attention_scores @ V, attention_scores"
      ],
      "metadata": {
        "id": "mFa7R6In-xT5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionWeighedV1(nn.Module):\n",
        "  def __init__(self, embedding_dim: int=3, output_dim: int=3, **kwargs):\n",
        "    super().__init__()\n",
        "    self.W_q = torch.nn.Parameter(torch.rand(embedding_dim, output_dim))\n",
        "    self.W_k = torch.nn.Parameter(torch.rand(embedding_dim, output_dim))\n",
        "    self.W_v = torch.nn.Parameter(torch.rand(embedding_dim, output_dim))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    assert len(inputs.shape) == 3\n",
        "    d = torch.tensor(inputs.shape[-1])\n",
        "    Q, K, V = inputs @ self.W_q, inputs @ self.W_k, inputs @ self.W_v\n",
        "    attention_scores = torch.softmax(Q @ K.transpose(-1, -2)/torch.sqrt(d), dim=-1)\n",
        "    return attention_scores @ V"
      ],
      "metadata": {
        "id": "-rjnHTF0QZnN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionWeighed(nn.Module):\n",
        "  def __init__(self, embedding_dim: int=3, output_dim: int=3, qkv_bias=False, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.W_q = nn.Linear(embedding_dim, output_dim, bias=qkv_bias)\n",
        "    self.W_k = nn.Linear(embedding_dim, output_dim, bias=qkv_bias)\n",
        "    self.W_v = nn.Linear(embedding_dim, output_dim, bias=qkv_bias)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    d = torch.tensor(inputs.shape[-1])\n",
        "    Q, K, V = self.W_q(inputs), self.W_k(inputs), self.W_v(inputs)\n",
        "    attention_scores = torch.softmax( Q @ K.transpose(-1, -2)/torch.sqrt(d) ,dim=-1)\n",
        "    return attention_scores @ V"
      ],
      "metadata": {
        "id": "xQ9x37_RSY37"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, embedding_dim: int=3, output_dim: int=3, context_length: int=6, dropout: float=0.1, qkv_bias=False, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.W_q = nn.Linear(embedding_dim, output_dim, qkv_bias)\n",
        "    self.W_k = nn.Linear(embedding_dim, output_dim, qkv_bias)\n",
        "    self.W_v = nn.Linear(embedding_dim, output_dim, qkv_bias)\n",
        "    self.d_out = torch.tensor(output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.mask = torch.triu(torch.ones(context_length, context_length, dtype=torch.bool), diagonal=1)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    Q, K, V = self.W_q(inputs), self.W_k(inputs), self.W_v(inputs)\n",
        "    attn_scores = Q @ K.transpose(-1, -2)/torch.sqrt(self.d_out)\n",
        "    attn_scores.masked_fill_(self.mask, -1e6)\n",
        "    attention_weights = torch.softmax(attn_scores, dim=-1)\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "    return attention_weights @ V"
      ],
      "metadata": {
        "id": "m9QZnm0koQqa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.stack([inputs, inputs])\n",
        "print(input.shape)\n",
        "l = CausalSelfAttention(3, 2)#SimplifiedSelfAttention()\n",
        "attention = l(input)\n",
        "print(attention.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQhwTtJ6-xaw",
        "outputId": "21dce260-7ca0-44ad-c93f-ee5eb5f0790e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n",
            "torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNwwjm99-xgj",
        "outputId": "32c33695-c24c-4b14-9d13-aaaac5630ad8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.3374, -0.4238,  0.2746],\n",
              "         [-0.2047, -0.3718,  0.3881],\n",
              "         [-0.1664, -0.3511,  0.4169],\n",
              "         [-0.1123, -0.3049,  0.3982],\n",
              "         [-0.1583, -0.2560,  0.3037],\n",
              "         [-0.1028, -0.2530,  0.3418]],\n",
              "\n",
              "        [[-0.3374, -0.4238,  0.2746],\n",
              "         [-0.2047, -0.3718,  0.3881],\n",
              "         [-0.1664, -0.3511,  0.4169],\n",
              "         [-0.1123, -0.3049,  0.3982],\n",
              "         [-0.1583, -0.2560,  0.3037],\n",
              "         [-0.1028, -0.2530,  0.3418]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4X2C0mr-xl3",
        "outputId": "60164db4-c410-4aaf-baa5-4c1342f0c3fd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999999999999999"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QcggRKsW-xrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0F1ay5DB-xwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimplifiedSelfAttention2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    d = torch.tensor(inputs.shape[-1])\n",
        "    Q, K, V = inputs, inputs, inputs\n",
        "    attention_weights = torch.softmax(Q @ K.transpose(-1, -2)/torch.sqrt(d), dim=-1)\n",
        "    attention = attention_weights @ V\n",
        "    return attention"
      ],
      "metadata": {
        "id": "4QbCgb_h7HHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionV1(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim):\n",
        "    super().__init__()\n",
        "    self.W_q = torch.nn.Parameter(torch.rand(in_dim, out_dim))\n",
        "    self.W_k = torch.nn.Parameter(torch.rand(in_dim, out_dim))\n",
        "    self.W_v = torch.nn.Parameter(torch.rand(in_dim, out_dim))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    Q = inputs @ self.W_q\n",
        "    K = inputs @ self.W_k\n",
        "    V = inputs @ self.W_v\n",
        "    d = torch.tensor(Q.shape[-1])\n",
        "    attention_weights = torch.softmax(Q @ K.transpose(-1, -2)/torch.sqrt(d), dim=-1)\n",
        "    attention = attention_weights @ V\n",
        "    return attention"
      ],
      "metadata": {
        "id": "JbR_iFgyJA65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionV2(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.W_q = torch.nn.Linear(in_dim, out_dim, bias=qkv_bias)\n",
        "    self.W_k = torch.nn.Linear(in_dim, out_dim, bias=qkv_bias)\n",
        "    self.W_v = torch.nn.Linear(in_dim, out_dim, bias=qkv_bias)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    Q = self.W_q(inputs)\n",
        "    K = self.W_k(inputs)\n",
        "    V = self.W_v(inputs)\n",
        "    d = torch.tensor(Q.shape[-1])\n",
        "    attention_weights = torch.softmax(Q @ K.transpose(-1, -2)/torch.sqrt(d), dim=-1)\n",
        "    attention = attention_weights @ V\n",
        "    return attention"
      ],
      "metadata": {
        "id": "ZZxrbnyjKRAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = SelfAttentionV2(3, 2)\n",
        "print(l(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_l8uGwJ7HNU",
        "outputId": "417d615f-8960-4bce-a212-68bbf46163d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2219, -0.1756],\n",
            "        [ 0.2166, -0.1714],\n",
            "        [ 0.2166, -0.1714],\n",
            "        [ 0.2109, -0.1654],\n",
            "        [ 0.2146, -0.1692],\n",
            "        [ 0.2110, -0.1656]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9YvMmB4z7HTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OSeNHFVP7HYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HFXoWgTj7HdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLayerNorm(nn.Module):\n",
        "  def __init__(self, normalized_dims, eps: float=1e-5):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.normalized_dims = [-x for x in range(1,normalized_dims+1)]\n",
        "    self.gamma = torch.nn.Parameter(torch.ones(1))\n",
        "    self.beta = torch.nn.Parameter(torch.zeros(1))\n",
        "\n",
        "  def forward(self, xb):\n",
        "    xb = (xb - xb.mean(dim=self.normalized_dims, keepdim=True)) / (xb.std(dim=self.normalized_dims, keepdim=True) + self.eps)\n",
        "    return self.gamma * xb + self.beta"
      ],
      "metadata": {
        "id": "ic7ZpWHROSpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim: int, eps: float=1e-5):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.nn.Parameter(torch.ones(emb_dim))\n",
        "    self.beta = torch.nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, xb):\n",
        "    xb = (xb - xb.mean(dim=-1, keepdims=True)) / torch.sqrt(xb.var(dim=-1, keepdims=True) + self.eps)\n",
        "    return xb * self.gamma + self.beta"
      ],
      "metadata": {
        "id": "sYgJ08BiVMTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeLU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.coeff = torch.sqrt(torch.tensor(2/math.pi))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5*x*(1.0 + torch.tanh( self.coeff * (x + 0.044715 * torch.pow(x, 3))))"
      ],
      "metadata": {
        "id": "7Phmg8Cia_r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    dim = cfg[\"emb_dim\"]\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(dim, 4 * dim),\n",
        "        GeLU(),\n",
        "        nn.Linear(4 * dim, dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, xb):\n",
        "    return self.layers(xb)"
      ],
      "metadata": {
        "id": "kLw2nPgEnpDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.att = MultiHeadAttention(d_in=cfg[\"emb_dim\"],\n",
        "                                  d_out=cfg[\"emb_dim\"],\n",
        "                                  context_length=cfg[\"context_length\"],\n",
        "                                  dropout=cfg[\"drop_rate\"],\n",
        "                                  num_heads=cfg[\"n_heads\"],\n",
        "                                  qkv_bias=cfg[\"qkv_bias\"])\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "    skip = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.drop_resid(x)\n",
        "    x = x + skip\n",
        "\n",
        "    skip = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_resid(x)\n",
        "    x = x + skip\n",
        "    return x"
      ],
      "metadata": {
        "id": "6111clTXrx1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "x = torch.rand(2, 4, 768) #A\n",
        "block = TransformerBlock(GPT_CONFIG_124M)\n",
        "output = block(x)\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p4wqvhSW2q4w",
        "outputId": "a3a4ace6-785b-4683-e36c-adf84265b972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (4) must match the size of tensor b (1024) at non-singleton dimension 3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-d38837588d98>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#A\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGPT_CONFIG_124M\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-15f5dd5c07bf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_resid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-a25c23aeafa0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mattention_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mcontext_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_weights\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (1024) at non-singleton dimension 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = GeLU()\n",
        "temp = torch.linspace(-5,5,100)\n",
        "t = g(temp)\n",
        "plt.plot(temp, t)\n",
        "plt.grid()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "k4LmKNaucRHz",
        "outputId": "f3b8d023-5e5a-42b0-ea8a-c2c70c2bd941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = torch.rand((3, 4, 6))\n",
        "LN = LayerNorm(6)\n",
        "t = LN(temp)\n",
        "print(t.mean(dim=-1))\n",
        "print(t.std(dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ofDA4xEP9h6",
        "outputId": "4d0ec4e1-0d2c-49e1-8d26-c502c551ac7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-4.9671e-08, -1.7881e-07,  0.0000e+00,  1.8626e-08],\n",
            "        [ 1.1921e-07, -5.9605e-08, -2.9802e-08, -7.2022e-08],\n",
            "        [-1.8378e-07, -1.6888e-07, -4.9671e-08, -7.9473e-08]],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "tensor([[0.9999, 0.9999, 1.0000, 0.9999],\n",
            "        [1.0000, 1.0000, 0.9999, 0.9999],\n",
            "        [0.9996, 0.9999, 1.0000, 0.9996]], grad_fn=<StdBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dims = 1 #-1, -2, -3\n",
        "[-x for x in range(1,dims+1)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKXA4yGQFLfk",
        "outputId": "df86f1c7-dab8-4ccc-a1b8-57916ee09402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-1]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5C6rVS8fLQv",
        "outputId": "2df177d8-019b-4173-8135-2843131b8e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}"
      ],
      "metadata": {
        "id": "64zCR0Qbgmdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyGPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "        *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "    )\n",
        "    self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x\n",
        "\n",
        "\n",
        "class DummyLayerNorm(nn.Module):\n",
        "  def __init__(self, normalized_shape, eps=1e-5):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x"
      ],
      "metadata": {
        "id": "zOdlifmvhAiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ffn = FeedForward(GPT_CONFIG_124M)\n",
        "x = torch.rand(2, 3, 768) #A\n",
        "out = ffn(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERBoUQ7HpHKD",
        "outputId": "a342de75-a90a-4eeb-9d06-9499b38deb7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYdZJvD-hAor",
        "outputId": "90498862-6dab-436e-e961-b3bae6e1e4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7db1dc132ff0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = DummyGPTModel(GPT_CONFIG_124M)"
      ],
      "metadata": {
        "id": "ewCVj2_2hAuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model(batch)\n",
        "print(logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROhsOB41hA1N",
        "outputId": "6b84643c-4a4c-4798-f3ac-3e984dc433af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N_GJp2Mx2qs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i5FKG1Rf2qzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dDdVGX0g2q5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum([0.1916, 0.1515, 0.1517, 0.1535, 0.1590, 0.1511])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2NIdMYZ2q-a",
        "outputId": "0381f8d6-8a57-4fb9-c7dd-2fa7ebf6b610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9584"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "[[0.43, 0.15, 0.89], # Your (x^1)\n",
        "[0.55, 0.87, 0.66], # journey (x^2)\n",
        "[0.57, 0.85, 0.64], # starts (x^3)\n",
        "[0.22, 0.58, 0.33], # with (x^4)\n",
        "[0.77, 0.25, 0.10], # one (x^5)\n",
        "[0.05, 0.80, 0.55]] # step (x^6)\n",
        ")"
      ],
      "metadata": {
        "id": "ip4Gvpeklxm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K, Q, V = inputs, inputs, inputs\n",
        "emb_dim = torch.tensor(K.shape[-1])\n",
        "attention_scores = torch.softmax(Q @ K.transpose(-1, -2)/torch.sqrt(emb_dim), dim=-1)\n",
        "attention = attention_scores @ V\n",
        "attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffIq--BAYQuz",
        "outputId": "cf0f59a2-d142-468e-e995-40f9afcbfc7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4374, 0.5896, 0.5582],\n",
              "        [0.4362, 0.6228, 0.5523],\n",
              "        [0.4370, 0.6216, 0.5515],\n",
              "        [0.4303, 0.6104, 0.5417],\n",
              "        [0.4525, 0.5874, 0.5274],\n",
              "        [0.4219, 0.6231, 0.5507]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = torch.randn((6,3))\n",
        "t1 = torch.softmax(temp, dim=-1)\n",
        "t1.sum(dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4-pgzyHYQ2J",
        "outputId": "a3e899b3-4d5e-4388-ff63-567a4970f8eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimplifiedSelfAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input):\n",
        "    emb_dim = torch.tensor(input.shape[-1])\n",
        "    self.Q = input\n",
        "    self.K = input\n",
        "    self.V = input\n",
        "    attention_scores = torch.softmax( self.Q @ self.K.transpose(-1, -2)/torch.sqrt(emb_dim), dim=-1)\n",
        "    attention = attention_scores @ self.V\n",
        "    return attention"
      ],
      "metadata": {
        "id": "H_jl8QHmYQ_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeighedSelfAttention(nn.Module):\n",
        "  def __init__(self, emd_dim: int):\n",
        "    super().__init__()\n",
        "    self.W_Q = nn.Parameter(torch.randn(emb_dim, emb_dim))\n",
        "    self.W_K = nn.Parameter(torch.randn(emb_dim, emb_dim))\n",
        "    self.W_V = nn.Parameter(torch.randn(emb_dim, emb_dim))\n",
        "\n",
        "  def forward(self, input):\n",
        "    emb_dim = torch.tensor(input.shape[-1])\n",
        "    self.Q = input @ self.W_Q\n",
        "    self.K = input @ self.W_K\n",
        "    self.V = input @ self.W_V\n",
        "    attention_scores = torch.softmax( self.Q @ self.K.transpose(-1, -2)/torch.sqrt(emb_dim), dim=-1)\n",
        "    attention = attention_scores @ self.V\n",
        "    return attention"
      ],
      "metadata": {
        "id": "gzqfDnbZqKM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = WeighedSelfAttention(3)\n",
        "s(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFIVcMJdYRHZ",
        "outputId": "2ed09fd5-b033-4596-d85e-9a03245421f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4198,  1.2207,  0.1922],\n",
              "        [-0.3068,  1.1415,  0.1918],\n",
              "        [-0.2993,  1.1426,  0.1934],\n",
              "        [-0.3271,  1.1560,  0.1918],\n",
              "        [-0.1270,  1.1806,  0.2328],\n",
              "        [-0.3838,  1.1448,  0.1790]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K, Q, V = inputs, inputs, inputs\n",
        "s, d = K.shape\n",
        "d_out = 2\n",
        "W_k = torch.nn.Parameter(torch.randn(d,d_out))\n",
        "W_q = torch.nn.Parameter(torch.randn(d,d_out))\n",
        "W_v = torch.nn.Parameter(torch.randn(d,d_out))\n",
        "\n",
        "Kf = K @ W_k\n",
        "Qf = Q @ W_q\n",
        "Vf = V @ W_v\n",
        "\n",
        "print(inputs.shape, Kf.shape, Qf.shape, Vf.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv-JXcr0l1o0",
        "outputId": "4f588e11-1d2c-4b00-877e-1e40933a1cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 3]) torch.Size([6, 2]) torch.Size([6, 2]) torch.Size([6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weights = torch.softmax(Qf @ Kf.T/np.sqrt(d_out), dim=1)\n",
        "out = attention_weights @ Vf\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz5WKZm_l1vO",
        "outputId": "9c95213d-1aaf-47b1-f55d-ba6c756c76a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.8478, -0.4353],\n",
              "        [ 0.8787, -0.4747],\n",
              "        [ 0.8790, -0.4737],\n",
              "        [ 0.8324, -0.4541],\n",
              "        [ 0.8551, -0.4416],\n",
              "        [ 0.8312, -0.4669]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(attention_weights.shape)\n",
        "attention_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhAV4xH2nen6",
        "outputId": "d98cd159-ebd7-4cf4-8b3c-fb6ff009dc8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 6])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2160, 0.0952, 0.0993, 0.1666, 0.3166, 0.1064],\n",
              "        [0.2699, 0.0462, 0.0499, 0.1324, 0.4443, 0.0573],\n",
              "        [0.2685, 0.0467, 0.0504, 0.1330, 0.4437, 0.0578],\n",
              "        [0.2461, 0.0886, 0.0925, 0.1594, 0.3134, 0.1000],\n",
              "        [0.2251, 0.0839, 0.0881, 0.1621, 0.3452, 0.0956],\n",
              "        [0.2655, 0.0778, 0.0817, 0.1528, 0.3327, 0.0896]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(attention_weights.detach())\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "04S5-6Qrl12u",
        "outputId": "842d7b4d-9572-486e-ba38-447cf5490e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x79e63c29a4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionV1(nn.Module):\n",
        "  def __init__(self, d_in, d_out):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_k = nn.Parameter(torch.randn(d_in,d_out))\n",
        "    self.W_q = nn.Parameter(torch.randn(d_in,d_out))\n",
        "    self.W_v = nn.Parameter(torch.randn(d_in,d_out))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    K = inputs @ self.W_k\n",
        "    Q = inputs @ self.W_q\n",
        "    V = inputs @ self.W_v\n",
        "\n",
        "\n",
        "    attention_weights = torch.softmax(Q @ K.T/self.d_out**0.5, dim=-1)\n",
        "    out = attention_weights @ V\n",
        "    return attention_weights, out"
      ],
      "metadata": {
        "id": "r42VcQYy6Ftb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionV2(nn.Module):\n",
        "  def __init__(self, d_in: int, d_out: int, kqv_bias: bool=False):\n",
        "    super().__init__()\n",
        "    self.W_k = nn.Linear(d_in, d_out, bias=kqv_bias)\n",
        "    self.W_q = nn.Linear(d_in, d_out, bias=kqv_bias)\n",
        "    self.W_v = nn.Linear(d_in, d_out, bias=kqv_bias)\n",
        "    self.d_out = d_out\n",
        "\n",
        "  def forward(self, inputs: torch.Tensor):\n",
        "    K = self.W_k(inputs)\n",
        "    Q = self.W_q(inputs)\n",
        "    V = self.W_v(inputs)\n",
        "\n",
        "    attention_weights = torch.softmax(Q @ K.T/self.d_out**0.5, dim=-1)\n",
        "    context_matrix = attention_weights @ V\n",
        "    return attention_weights, context_matrix"
      ],
      "metadata": {
        "id": "0qFkjrFM_D9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttentionV1(nn.Module):\n",
        "  def __init__(self, d_in: int, d_out: int, kqv_bias: bool=False):\n",
        "    super().__init__()\n",
        "    self.W_k = nn.Linear(d_in, d_out, bias=kqv_bias)\n",
        "    self.W_q = nn.Linear(d_in, d_out, bias=kqv_bias)\n",
        "    self.W_v = nn.Linear(d_in, d_out, bias=kqv_bias)\n",
        "    self.d_out = d_out\n",
        "\n",
        "  def forward(self, inputs: torch.Tensor):\n",
        "    K = self.W_k(inputs)\n",
        "    Q = self.W_q(inputs)\n",
        "    V = self.W_v(inputs)\n",
        "\n",
        "    seq_len = inputs.shape[0]\n",
        "    attention_scores = Q @ K.T\n",
        "    attention_scores[torch.triu(torch.ones_like(attention_scores, dtype=torch.bool), diagonal=1)] = -1e3\n",
        "    attention_weights = torch.softmax(attention_scores/self.d_out**0.5, dim=-1)\n",
        "    context_matrix = attention_weights @ V\n",
        "    return attention_weights, context_matrix"
      ],
      "metadata": {
        "id": "RzAFIcePM12J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self, d_in: int, d_out: int, context_length: int, dropout: float, qkv_bias: bool =False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer('mask', torch.triu(torch.ones((context_length, context_length), dtype=torch.bool), diagonal=1))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    K = self.W_k(inputs)\n",
        "    Q = self.W_q(inputs)\n",
        "    V = self.W_v(inputs)\n",
        "    attention_scores = Q @ K.transpose(1,2)\n",
        "    attention_scores.masked_fill_(self.mask, -torch.inf)\n",
        "    attention_weights = self.dropout(torch.softmax(attention_scores/self.d_out**0.5, dim=-1))\n",
        "    context_matrix = attention_weights @ V\n",
        "    return context_matrix"
      ],
      "metadata": {
        "id": "2BjyBrXVjkLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    return torch.cat([head(inputs) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "xOsRoMVQXiVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert d_out % num_heads == 0\n",
        "\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads\n",
        "    self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_out, d_out)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer('mask', torch.triu(torch.ones((context_length, context_length), dtype=torch.bool), diagonal=1))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    batches, num_tokens, d_in = inputs.shape\n",
        "    K = self.W_k(inputs)\n",
        "    Q = self.W_q(inputs)\n",
        "    V = self.W_v(inputs)\n",
        "\n",
        "    K = K.view(batches, num_tokens, self.num_heads, self.head_dim)\n",
        "    Q = Q.view(batches, num_tokens, self.num_heads, self.head_dim)\n",
        "    V = V.view(batches, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    K = K.transpose(1, 2)\n",
        "    Q = Q.transpose(1, 2)\n",
        "    V = V.transpose(1, 2)\n",
        "\n",
        "    attention_scores = Q @ K.transpose(2, 3)\n",
        "    attention_scores.masked_fill_(self.mask, -torch.inf)\n",
        "    attention_weights = self.dropout(torch.softmax(attention_scores/K.shape[-1]**0.5, dim=-1))\n",
        "    context_matrix = (attention_weights @ V).transpose(1, 2)\n",
        "\n",
        "    context_matrix = context_matrix.contiguous().view(batches, num_tokens, self.d_out)\n",
        "\n",
        "    return self.out_proj(context_matrix)"
      ],
      "metadata": {
        "id": "0uNNGd_wi-tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batched_inputs = torch.stack([inputs, inputs], dim=0)\n",
        "print(batched_inputs.shape)\n",
        "l = CausalAttention(3, 2, 6, 0.5, False)\n",
        "c = l(batched_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "FDIY-SWIBOBA",
        "outputId": "5be3b47f-b2ae-4708-9671-40691233b569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'inputs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-a26bb6ad4e5d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatched_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCausalAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l4 = MultiHeadAttention(3, 12, 6, 0.5, 4)\n",
        "c = l4(batched_inputs)\n",
        "c.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwG7FSEeaTFQ",
        "outputId": "e7d8ef1b-d546-4af5-f9ad-d4f6dd4eacfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 12])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones((6, 6), dtype=torch.bool), diagonal=1)\n",
        "w.masked_fill_(mask, -torch.inf)\n",
        "w"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTYpbUdFJMkW",
        "outputId": "d5724781-42e0-4278-e6db-7e53e63b0c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.3389,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "         [0.0000, 0.3070,   -inf,   -inf,   -inf,   -inf],\n",
              "         [0.3566, 0.3082, 0.0000,   -inf,   -inf,   -inf],\n",
              "         [0.0000, 0.0000, 0.0000, 0.3444,   -inf,   -inf],\n",
              "         [0.3295, 0.0000, 0.0000, 0.3319, 0.0000,   -inf],\n",
              "         [0.0000, 0.0000, 0.3023, 0.0000, 0.3370, 0.3446]],\n",
              "\n",
              "        [[0.0000,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "         [0.3577, 0.3070,   -inf,   -inf,   -inf,   -inf],\n",
              "         [0.0000, 0.0000, 0.3083,   -inf,   -inf,   -inf],\n",
              "         [0.3510, 0.0000, 0.0000, 0.0000,   -inf,   -inf],\n",
              "         [0.0000, 0.3370, 0.0000, 0.0000, 0.3313,   -inf],\n",
              "         [0.3629, 0.0000, 0.3023, 0.3511, 0.0000, 0.0000]]],\n",
              "       grad_fn=<MaskedFillBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = inputs @ inputs.T\n",
        "print(temp)\n",
        "mask = torch.triu(torch.ones_like(temp, dtype=torch.bool), diagonal=1)\n",
        "temp[mask] = 0.0\n",
        "print(temp)\n",
        "print(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoBvHTkyVKaU",
        "outputId": "fa24f077-9518-4cc4-f49d-080b2d809e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
            "tensor([[0.9995, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.9544, 1.4950, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.9422, 1.4754, 1.4570, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.0000, 0.0000],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.0000],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
            "tensor([[False,  True,  True,  True,  True,  True],\n",
            "        [False, False,  True,  True,  True,  True],\n",
            "        [False, False, False,  True,  True,  True],\n",
            "        [False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False,  True],\n",
            "        [False, False, False, False, False, False]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(out.shape)\n",
        "torch.softmax(out, dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-hS9nG4p9Yu",
        "outputId": "8b1aa570-9781-40d3-fdda-acadf8ad8069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 3])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3037, 0.3536, 0.3427],\n",
              "        [0.3005, 0.3621, 0.3375],\n",
              "        [0.3008, 0.3618, 0.3373],\n",
              "        [0.3016, 0.3612, 0.3372],\n",
              "        [0.3104, 0.3552, 0.3345],\n",
              "        [0.2976, 0.3639, 0.3385]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum([0.1916, 0.1515, 0.1517, 0.1535, 0.1590, 0.1511])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znSdUH7OmxeY",
        "outputId": "53f4ebbe-7049-4612-d501-3ba8690b1249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9584"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 4\n",
        "valid_len = torch.tensor([4,2,3])"
      ],
      "metadata": {
        "id": "uFkr3Q3QBDkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 3\n",
        "temp1 = torch.arange(3)[None, :]\n",
        "temp2 = torch.arange(3)[:, None]\n",
        "temp3 = torch.randn((3,3))\n",
        "print(temp3)\n",
        "mask = temp1 > temp2\n",
        "temp3[torch.arange(seq_len)[None, :] > torch.arange(seq_len)[:, None]] = 0\n",
        "print(temp3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB9dFIxyNiDb",
        "outputId": "56b12ed7-a576-4526-9025-5e90acc4ef32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2713,  0.3433, -0.9766],\n",
            "        [-0.2188, -1.7379, -1.0785],\n",
            "        [ 0.3094, -2.7422, -0.1531]])\n",
            "tensor([[ 1.2713,  0.0000,  0.0000],\n",
            "        [-0.2188, -1.7379,  0.0000],\n",
            "        [ 0.3094, -2.7422, -0.1531]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.arange((maxlen), dtype=torch.float32,)[None, :] < valid_len[:, None]\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn7P9wQeA79j",
        "outputId": "ba4242e3-28bd-49bd-8cdb-2ac24b8a2211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ True,  True,  True,  True],\n",
              "        [ True,  True, False, False],\n",
              "        [ True,  True,  True, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[~mask] = 0.0\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feBKJ9uHB6GY",
        "outputId": "a3aa8110-b23c-49cf-bde0-f0829663139f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.6233, -0.6503, -1.2038, -0.5545, -1.9048,  1.2204,  1.8532,\n",
              "          -0.9074,  0.1345, -2.9428, -0.7558, -1.2853,  0.8911, -0.5736,\n",
              "          -0.2995,  0.9408],\n",
              "         [ 0.4482, -0.2003, -0.9887,  0.4010, -0.1182,  1.3910, -2.0211,\n",
              "          -0.6343, -1.9523, -0.1101, -0.1928, -0.2492, -0.4320,  0.3045,\n",
              "           0.2017,  2.9055],\n",
              "         [-0.2371,  0.3748, -0.5849,  0.9120, -0.7012,  0.9667, -1.7764,\n",
              "          -1.4355,  0.7844, -0.8367,  0.0492, -1.0133,  0.5697,  2.2852,\n",
              "          -1.6383,  1.4620],\n",
              "         [-0.7184, -1.6823, -1.2836,  1.0752, -0.0149, -1.1653, -0.2610,\n",
              "           0.3477,  0.4714, -0.4765,  1.2675,  0.2631,  1.5908, -0.7556,\n",
              "           1.1149, -1.1580]],\n",
              "\n",
              "        [[ 0.2296,  1.8949,  0.0090,  0.0465,  0.9412,  0.3783, -0.6743,\n",
              "          -0.6760,  0.5918, -0.1773, -0.5809, -0.4002, -0.0376, -1.1544,\n",
              "          -0.2282,  1.6922],\n",
              "         [ 0.3133,  0.5159,  0.0518, -0.5467,  0.2857, -0.3032, -0.0162,\n",
              "          -0.4138, -0.1958, -1.0138, -0.9194, -1.1076, -0.2089, -0.4665,\n",
              "          -0.3244, -3.0922],\n",
              "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "           0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "           0.0000,  0.0000]],\n",
              "\n",
              "        [[-0.9956,  0.6388,  0.6169, -0.3421, -0.3528,  0.1620, -0.6167,\n",
              "           0.7436, -0.5971,  2.1118,  1.0127, -1.9101,  0.0074,  0.8424,\n",
              "           1.0287, -0.5215],\n",
              "         [-0.0097, -0.6020,  0.5968,  0.5960,  0.3092,  0.4594, -0.3396,\n",
              "          -1.9863, -0.6011, -1.3319, -0.0093, -0.6883, -1.8543, -0.1497,\n",
              "           1.0284, -0.4947],\n",
              "         [-1.5743,  0.0283,  0.7732,  0.5790,  0.0223, -2.2100, -0.5853,\n",
              "          -0.6258, -1.3343, -1.1101, -0.9495,  0.3144, -0.5310, -0.4237,\n",
              "           1.6078,  0.0144],\n",
              "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "           0.0000,  0.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.randn((3,4,16))\n",
        "maxlen = X.size(1)\n",
        "maxlen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQeQ2txaBOLY",
        "outputId": "1250b818-bd20-4da5-ad4c-2e593d6a245b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_len[:, None]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4tRdzqmBc00",
        "outputId": "860d4593-1da4-4efa-dbf2-d883d2b58876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4],\n",
              "        [2],\n",
              "        [3]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "math.pi"
      ],
      "metadata": {
        "id": "0c0qHxESBx_R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "outputId": "3824bdb7-11b2-4928-8d78-27efbc55d1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'math' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c49acc181da4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'math' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kDn54H8abs3D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}